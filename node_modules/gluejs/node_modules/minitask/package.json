{
  "name": "minitask",
  "version": "0.0.1",
  "description": "A standard/convention for running tasks over a list of files based around Node core streams2",
  "main": "index.js",
  "scripts": {
    "test": "node test/cache.test.js"
  },
  "peerDependencies": {
    "minilog": "2.x"
  },
  "devDependencies": {
    "minilog": "2.x"
  },
  "repository": {
    "type": "git",
    "url": "git://github.com/mixu/minitask.git"
  },
  "keywords": [
    "task",
    "tasks",
    "build",
    "job",
    "run",
    "make"
  ],
  "author": {
    "name": "Mikito Takada",
    "email": "mikito.takada@gmail.com",
    "url": "http://mixu.net/"
  },
  "license": "BSD",
  "readmeFilename": "readme.md",
  "gitHead": "de68d793f30add8746b519c3e0fde3f54d484228",
  "readme": "# minitask\n\nA standard/convention for running tasks over a list of files based around Node core streams2.\n\nCompatible with Node 0.8.x as well thanks to readable-stream by isaacs.\n\n## Introduction\n\n[grunt](http://gruntjs.com/) is the Javascript task runner that's most popular, but I mostly prefer using makefiles since they require less ceremony.\n\nHowever, sometimes you want to express something as an operation applied to a list of files, while keeping the ability to plug in more tasks via unix pipes and custom functions. That's what this is, a simple convention for working on a list of files using constructs from the Node core.\n\nminitask is not a makefile replacement, it is a convention for writing things that apply a bunch of pipes to a list of files.\n\nminitask doesn't even define any new APIs (unlike, say, [node-task](https://github.com/node-task/spec/wiki), which is destined to become grunt's next set of internals which seems to implement their own (!) synchronous (!!) version of core streams). In minitask, everything is just based on using [Node core streams](http://nodejs.org/api/stream.html) in a specific way and structuring your code into reusable tasks. The minitask repo is a bunch of functions that support those conventions.\n\n## The first step: creating and annotating the list of files\n\nEach minitask starts with a list of files, which simply an object that looks like this:\n\n    {\n      files: [\n        { name: '/full/path/to/file.js' }\n      ]\n    }\n\nThe minitask core API has a file iterator that can build these lists for consumption, given path specifications as inputs.\n\nThis array of files is then filtered an annotated using list tasks, which are functions. For example, `filter-git.js`:\n\n````javascript\n// filter-git-directories: a list task that filters out .git directories from the list\nmodule.exports = function(list) {\n  list.files = list.files.filter(function(item) {\n    return !item.name.match(new RegExp('/\\.git/'));\n  });\n};\n````\n\nList tasks are basically any tasks that include / exclude or otherwise work on metadata.\n\nTo add metadata, you should add properties either to each file, or to the list object itself. For example, `annotate-stat.js`:\n\n````javascript\nvar fs = require('fs');\n\n// This task adds a .stat property to every file in the list\nmodule.exports = function(list) {\n  list.files.forEach(function(item, i) {\n    list.files[i].stat = fs.statSync(item.name);\n  });\n};\n````\n\nThe key benefit of separating tasks such as filtering and annotating metadata into a step that occurs after the list of files is created is that it makes those tasks easier to reuse and test. Previously, I would perform filtering at the same time as I was reading in the file tree. The problem with doing both filtering and file tree iteration is that you end up with some unchangeable filtering logic that's embedded inside your file iterator.\n\nHaving your filtering and annotation embedded in the file iterator gets really annoying in some cases: for example, for [gluejs](http://mixu.net/gluejs/) there are multiple filtering rules: package.json files, .npmignore files and user-specified rules. Those were applied in various separate components that basically excluded some paths from traversal based on custom logic.\n\nRather than special casing and doing two things at the same time, with minitask you read in a file tree and then all filters work on the same structure: an array of paths with metadata. Since filtering is a operation that's separate from reading in the initial tree, it's much easier to see and configure what gets excluded and to define new metadata -related operations.\n\n## Defining tasks that operate on files (= streams)\n\nFile tasks are the other type of task.\n\nThere are three different alternatives, corresponding to different native APIs:\n\n- streams: returning an object with { stdout: ..., stdin: ... }\n- async calls: returning a function of arity 2: function(onEach, onDone) {}\n\n\nThey use the Node 0.10.x stream interface based on a convention that makes using child_process.spawn particularly easy:\n\n````javascript\n// uglify-task: runs uglify\nvar spawn = require('child_process').spawn;\nmodule.exports = function(options) {\n  var task = spawn('uglifyjs', ['--no-copyright']);\n  task.on('exit', function(code) {\n    task.emit('error', 'Child process exited with nonzero exit code: '+ code);\n  });\n  return task;\n};\n````\n\nYou have to return:\n\n- an object with two streams: { stdin: WritableStream, stdout: ReadableStream }\n- or a function that when called returns an object with the stdin and stdout properties\n\nNote that child_process.spawn() returns exactly the right kind of object.\n\nThe key here is that every file task is a Node 0.10.x stream. Streams are easy to compose together via pipe(), and all I/O objects in Node are streams. This makes it easy to compose file tasks and to redirect them to different places.\n\nIf you're doing a JS-based stream transformation, then you can return a instance of Node core's [stream.Transform](stream.Transform) duplex stream, wrapped to look like a process:\n\n````javascript\n// use readable-stream to use Node 0.10.x streams in Node 0.8.x\nvar Transform = require('readable-stream').Transform;\n\nfunction Wrap(options) {\n  Transform.call(this, options);\n  this.first = true;\n}\n\n// this is just the recommended boilerplate from the Node core docs\nWrap.prototype = Object.create(Transform.prototype, { constructor: { value: Wrap }});\n\nWrap.prototype._transform = function(chunk, encoding, done) {\n  if(this.first) {\n    this.push('!!');\n    this.first = false;\n  }\n  this.push(chunk);\n  done();\n};\n\nWrap.prototype._flush = function(done) {\n  this.push('!!');\n  done();\n};\n\nmodule.exports = function(options) {\n  var instance = new Wrap(options);\n  // since it's a duplex stream, let the stdin and stdout point to the same thing\n  return {\n    stdin: instance,\n    stdout: instance\n  };\n};\n````\n\nThis also means that any 3rd party code that implements on `stream.Transform` is immediately usable with just a wrapping function that creates a new instance.\n\n## Running tasks\n\nThe last piece of minitask is the runner.\n\nThe runner is the last task, it is responsible for using list tasks and file tasks to achieve whatever it wants. There are no strong requirements here; it's not worth it to really try to standardize the runner in my opinion - the overhead of dealing with some kind of standard for expressing a workflow is less than the benefits of reuse. Whatever can be reused should be extracted into file tasks and list tasks and the runner is everything that can't be reused.\n\nThe first parameter is the list structure of files, without any filters or tasks applied to it.\n\n````javascript\n// serve-index:\nvar http = require('http');\n\nmodule.exports = function(list, options) {\n  http.createServer(function(req, res) {\n    if(req.url == '/') {\n      res.end('<html><ul><li>'+ tree.files.join('</li><li>') +'</li></ul></html>');\n    } else {\n      res.end('Unknown: ' + req.url);\n    }\n  }).listen(8000).on('listening', function() {\n    console.log('Listening on localhost:8000');\n  });\n};\n````\n\nThe runner is king, it gets to decide what to do with the tree and options it's supplied.\n\n## API docs\n\nThe minitask core basically defines a set of helpers that support these convetions:\n\n- `list.js` is the thing that iterates paths and returns a file list array for further consumption\n- `runner.js` is a function that applies a set of file tasks on a readable stream and returns a writable stream\n\nTODO: document the list\n\nTODO: specify how the list should be annotated with tasks\n\n### Runner API\n\nThe runner is a helper method that takes an input stream (e.g. an object { stdout: ... }), an array of tasks and a done function. It instantiates tasks if necessary, and pipes the tasks together, and ensures that the last task in the pipeline calls the done function.\n\nUsage example:\n\n    var runner = require('minitask').runner,\n        tasks = [ fileTask, ... ];\n\n    var last = runner({ stdout: fs.createReadStream(filename) }, tasks, function() {\n      console.log('done');\n    });\n    // need to do this here so we can catch the second-to-last stream's \"end\" event;\n    last.stdout.pipe(process.stdout, { end: false });\n\n\n## Caching\n\nFile processing tasks such as package builds and metadata reads are often run multiple times. It is useful to cache the output from these tasks and only re-run the processing when a file has changed. GNU Make, for example, relies on dependency resolution + file last modified timestamps to skip work where possible.\n\nA cacheable task is any task that reads a specific file path and writes to a writable stream at the end.\n\nThe caching system can either use a md5 hash, or the last modified+file size information to determine whether a task needs to be re-run. Additionally, an options hash can be passed to take into account different additional options.\n\nWhen the caching system is used, the task output is additionally written to a separate file. The assumption here is that each file task (with a task options hash and input md5) performs the same deterministic transformation. When the current input file's md5 and task options hash match, then the previously written cached result is streamed directly rather than running the full stack of transformations.\n\n### Cache API\n\nThe cache API looks a lot like the runner API, but it requires an explicit file path and options hash.\n\n    var last = cache({ filepath: filepath, cachepath: ..., md5: ..., stat: ..., options: ... }, tasks, function() {\n\n    });\n\n\n## Command line tool\n\n",
  "_id": "minitask@0.0.1",
  "_from": "minitask@0.0.1"
}
